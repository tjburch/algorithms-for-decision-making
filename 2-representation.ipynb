{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "painful-direction",
   "metadata": {},
   "source": [
    "# Representation\n",
    "\n",
    "This chapter will formalize uncertainty representation, through the notion of degree of belief, distributions, and how those distributions interact.\n",
    "\n",
    "## 2.1 Degrees of Belief and Probability\n",
    "\n",
    "Using A≻B to represent \"A is more plausible than B.\" Through this you can get many baseline axioms - universal comparibility and transitivity. From these, $P(A) > P(B)$ iff $A≻B$\n",
    "\n",
    "## 2.2 Probability Distributions\n",
    "\n",
    "### 2.2.1 Discrete Probability Distributions\n",
    "\n",
    "Represented as a _probability mass function_ (pmf), assigning a probability to every possible assigment of its input variable to a value. Masses must sum to one.\n",
    "\n",
    "Notation - $P(x^3)$ is an assignment, equivalent to $P(X=3)$.\n",
    "\n",
    "Parameters of a distribution govern the probabilities. For example a dice roll has 6 parameters, but only 5 independent, since the last can be constrained.\n",
    "\n",
    "### 2.2.2 Continuous Probability Distributions\n",
    "\n",
    "Instead of a pmf, we use a _probability distribution function_ (pdf), which integrates to 1. Another way to represent it is a _cumulative distribution function_ (cdf), which can be defined as\n",
    "\n",
    "$$\n",
    "\\text{cdf}_X (x) = P(X \\leq x) = \\int_{-\\inf}^{x} p(x')dx'\n",
    "$$\n",
    "\n",
    "Related is the _quantile function_ or _inverse cumulative distribution function_. The value of $\\text{quantile}_X (\\alpha)$ is the value $x$ such that $P(X \\leq x) = \\alpha$.\n",
    "\n",
    "\n",
    "**Some distributions:**\n",
    "\n",
    "Uniform, $\\mathcal{U}(a,b)$. Assigns probability density uniformly between two points, giving pdf of $p(x) = 1/(b-a)$. \n",
    "\n",
    "Gaussian or Normal distribution, $\\mathcal{N}(\\mu, \\sigma^2)$, defined by mean and variance. The pdf at $x$ is $\\mathcal{N}(x|\\mu,\\sigma^2) = \\frac{1}{\\sigma}\\phi \\big(\\frac{x-\\mu}{\\sigma} \\big)$, for $\\phi(x) = \\frac{1}{2\\pi} \\exp \\big(-\\frac{x^2}{2} \\big)$\n",
    "\n",
    "Gaussians are convienient because they have few parameters, however assign probability to large negative and positive values. To avoid this, you could use a truncated Gaussian. $\\mathcal{N}(x| \\mu, \\sigma^2, a, b) = \\frac{ \\frac{1}{\\sigma}\\phi \\big(\\frac{x-\\mu}{\\sigma} \\big)}{\\Phi (\\frac{b-\\mu}{\\sigma}) - \\Phi (\\frac{a-\\mu}{\\sigma})}$\n",
    "\n",
    "The _support_ of a distribution is where it has non-zero values.\n",
    "\n",
    "The gaussian is unimodal by itself. Multimodal phenomenon can be represented in different ways, one being a gaussian mixture model, a weighted average of different gaussians. $p(x| \\mu_{1:n}, \\sigma^2_{1:n}, \\rho_{1:n}) = \\sum_{i=1}^{n} \\rho_i \\mathcal{N}(x|\\mu_i, \\sigma^2_i)$\n",
    "\n",
    "Another option is to represent the distribution over a continuous variable as a piecewise-uniform density.\n",
    "\n",
    "## 2.3 Joint Distributions\n",
    "\n",
    "A _joint distribution_ is a probability distribution over multiple variables. A distribution over a single variable is called a univariate distribution, over multiple is multivariate.\n",
    "\n",
    "If over two discrete variables $X$ and $Y$, then $P(x,y)$ is the probability that $X=x$ and $Y=y$. The _marginal_ distribution of a variable or set of variables can be found by summing the others (law of total probability):\n",
    "\n",
    "$$\n",
    "P(x) = \\sum_y P(x,y)\n",
    "$$\n",
    "\n",
    "### 2.3.1 Discrete Joint Distributions\n",
    "\n",
    "The joint distribution of two discrete variables can be shown as a table, enumerating all possible assignments. For $n$ binary variables, need $2^n-1$ independent parameters to specify the joint distribution (e.g. 3 variables can be configured 8 ways, the last configuration is 1-$\\theta_{1:7}$).\n",
    "\n",
    "In some cases, we can assume variables are independent ($X\\perp Y$), in that case $P(x,y) = P(x)P(y)$. For $n$ binaries, $P(x_{1:n}) = \\prod_i P(x_i)$, and the number of paramteres is $n$. This is often a poor assumption, but conversely saves complexity.\n",
    "\n",
    "Joint distributions can be represented by _factors_, $\\phi$. A factor over a set of variables is a function from assignments to real numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "exterior-marshall",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An Algorithmic implementation of discrete factors\n",
    "include(\"util_funcs.jl\")\n",
    "\n",
    "struct Variable\n",
    "    name::Symbol\n",
    "    m::Int # number of possible values\n",
    "end\n",
    "\n",
    "const Assignment = Dict{Symbol, Int}\n",
    "const FactorTable = Dict{Assignment, Float64}\n",
    "\n",
    "struct Factor\n",
    "    vars::Vector{Variable}\n",
    "    table::FactorTable\n",
    "end\n",
    "\n",
    "variablenames(φ::Factor) = [var.name for var in φ.vars]\n",
    "\n",
    "select(a::Assignment, varnames::Vector{Symbol}) = Assignment(n=>a[n] for n in varnames)\n",
    "\n",
    "function assignments(vars::AbstractVector{Variable})\n",
    "    names = [var.name for var in vars]\n",
    "    return vec([Assignment(n=>v for (n,v) in zip(names,values)) \n",
    "            for values in product((1:v.m for v in vars)...)])\n",
    "end\n",
    "\n",
    "function normalize!(φ::Factor)\n",
    "    z = sum(p for  (a,p) in φ.table)\n",
    "    for (a,p) in φ.table\n",
    "        φ.table[a] = p/z\n",
    "    end\n",
    "    return φ\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "absolute-kidney",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Factor(Variable[Variable(:x, 2), Variable(:y, 2), Variable(:z, 2)], Dict(Dict(:y => 1,:z => 2,:x => 2) => 0.05,Dict(:y => 2,:z => 2,:x => 2) => 0.07,Dict(:y => 1,:z => 1,:x => 1) => 0.08,Dict(:y => 2,:z => 1,:x => 1) => 0.09,Dict(:y => 1,:z => 2,:x => 1) => 0.31,Dict(:y => 1,:z => 1,:x => 2) => 0.01,Dict(:y => 2,:z => 2,:x => 1) => 0.37,Dict(:y => 2,:z => 1,:x => 2) => 0.02))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiating a table:\n",
    "X = Variable(:x, 2)\n",
    "Y = Variable(:y, 2)\n",
    "Z = Variable(:z, 2)\n",
    "φ = Factor([X,Y,Z], FactorTable(\n",
    "        (x=1, y=1, z=1) => 0.08, \n",
    "        (x=1, y=1, z=2) => 0.31,\n",
    "        (x=1, y=2, z=1) => 0.09, \n",
    "        (x=1, y=2, z=2) => 0.37,\n",
    "        (x=2, y=1, z=1) => 0.01, \n",
    "        (x=2, y=1, z=2) => 0.05,\n",
    "        (x=2, y=2, z=1) => 0.02, \n",
    "        (x=2, y=2, z=2) => 0.07,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recent-blade",
   "metadata": {},
   "source": [
    "Another approach is to use a _decision tree_, which is most effective when there are many variables and many repeated values.\n",
    "\n",
    "### 2.3.2 Continuous Joint Distributions\n",
    "\n",
    "Similar approach can be taken for continuous variables.\n",
    "\n",
    "A simple distribution is the _multivariate uniform distribution_, $\\mathcal{U}(\\mathbf{a}, \\mathbf{b})$ is a uniform distribution over a box.\n",
    "\n",
    "A mixture model can be created from a weighted collection of MV uniform distributions - for $n$ variables and $k$ mixture components, need $k(2n+1) - 1$ independent parameters.\n",
    "\n",
    "It is common to represent piecewise constant density functions by discretizing each variable independently - represented by a set of bin edges for each variable. For $n$ variables and $m$ bins for each, need $m^n -1$ parameters to define the distribution.\n",
    "\n",
    "Another useful distribution is the mutlivariate gaussian distribution:\n",
    "\n",
    "$$\n",
    "\\mathcal{N}(\\mathbf{x} | \\mathbf{\\mu}, \\mathbf{\\Sigma}) = \\frac{1}{(2\\pi)^{n/2} |\\mathbf{\\Sigma}|^{1/2}} \\exp \\big( -\\frac{1}{2} (\\mathbf{x}-\\mathbf{\\mu}^T \\mathbf{\\Sigma}^{-1} (\\mathbf{x} - \\mathbf{\\mu}) \\big)\n",
    "$$\n",
    "\n",
    "$\\mathbf{x}$ is in $\\mathbf{R}^n$ called _mean vector_, and $\\mathbf{\\Sigma}$ is the _covariance matrix_. This has $n + (n+1)n/2$ independent paramters - components in $\\mu$ added to the components of the upper traingle of the covariance matrix. You can also define multivariate gaussian mixture models.\n",
    "\n",
    "## 2.4 Conditional Distributions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "restricted-budget",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.5.3",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
